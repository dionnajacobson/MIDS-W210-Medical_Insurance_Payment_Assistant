{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept - Predict Top-5 diagnosis categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul> <li> D1: Diseases of the circulatory system </ul> </li>\n",
    "<ul> <li> D2: External causes of injury and supplemental classification </ul> </li>\n",
    "<ul> <li> D3: Endocrine, nutritional and metabolic diseases, and immunity disorders </ul> </li>\n",
    "<ul> <li> D4: Diseases of the respiratory system </ul> </li>\n",
    "<ul> <li> D5: Injury and poisoning </ul> </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre process data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import collect_list\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud\n",
    "#from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"ClickThrough\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the S3 URL as per your config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_PREFIX = 's3://'\n",
    "S3_BUCKET = '<Add your bucket>'\n",
    "S3_PATH = '<Add your folder path>'\n",
    "S3_URL = S3_PREFIX + S3_BUCKET + S3_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_LIST = ['D_ICD_DIAGNOSES.csv','DIAGNOSES_ICD.csv','NOTEEVENTS.csv']\n",
    "for filename_LIST_ITEM in filename_LIST:\n",
    "    filename,fileformat = filename_LIST_ITEM.split('.')\n",
    "    exec(filename+'_DF = sqlContext.read.format(\"'+fileformat+'\").option(\"header\", \"true\").option(\"multiline\",True).'+\n",
    "         'option(\"escape\",'+\"'\"+'\"'+\"')\"+'.load(\"'+S3_URL+'/'\n",
    "         +filename+'.csv\")')\n",
    "    exec(filename+'_DF.createOrReplaceTempView(\"'+filename+'\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Build HADM level diagnosis flag tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select A.*, B.SHORT_TITLE, B.LONG_TITLE, \n",
    "case when substr(A.ICD9_CODE,1,1) in ('E','V') then 'external causes of injury and supplemental classification'\n",
    "when substr(A.ICD9_CODE,1,3) between 001 and 139 then 'infectious and parasitic diseases'\n",
    "when substr(A.ICD9_CODE,1,3) between 140 and 239 then 'neoplasms'\n",
    "when substr(A.ICD9_CODE,1,3) between 240 and 279 then 'endocrine, nutritional and metabolic diseases, and immunity disorders'\n",
    "when substr(A.ICD9_CODE,1,3) between 280 and 289 then 'diseases of the blood and blood-forming organs'\n",
    "when substr(A.ICD9_CODE,1,3) between 290 and 319 then 'mental disorders'\n",
    "when substr(A.ICD9_CODE,1,3) between 320 and 389 then 'diseases of the nervous system and sense organs'\n",
    "when substr(A.ICD9_CODE,1,3) between 390 and 459 then 'diseases of the circulatory system'\n",
    "when substr(A.ICD9_CODE,1,3) between 460 and 519 then 'diseases of the respiratory system'\n",
    "when substr(A.ICD9_CODE,1,3) between 520 and 579 then 'diseases of the digestive system'\n",
    "when substr(A.ICD9_CODE,1,3) between 580 and 629 then 'diseases of the genitourinary system'\n",
    "when substr(A.ICD9_CODE,1,3) between 630 and 679 then 'complications of pregnancy, childbirth, and the puerperium'\n",
    "when substr(A.ICD9_CODE,1,3) between 680 and 709 then 'diseases of the skin and subcutaneous tissue'\n",
    "when substr(A.ICD9_CODE,1,3) between 710 and 739 then 'diseases of the musculoskeletal system and connective tissue'\n",
    "when substr(A.ICD9_CODE,1,3) between 740 and 759 then 'congenital anomalies'\n",
    "when substr(A.ICD9_CODE,1,3) between 760 and 779 then 'certain conditions originating in the perinatal period'\n",
    "when substr(A.ICD9_CODE,1,3) between 780 and 799 then 'symptoms, signs, and ill-defined conditions'\n",
    "when substr(A.ICD9_CODE,1,3) between 800 and 999 then 'injury and poisoning' \n",
    "end as ICD_GROUP\n",
    "from DIAGNOSES_ICD A\n",
    "left join D_ICD_DIAGNOSES B\n",
    "on A.ICD9_CODE = B.ICD9_CODE\n",
    "\"\"\").createOrReplaceTempView('DIAGNOSES_ICD_WITH_GROUPING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGNOSIS_GROUPING_DF = spark.sql(\"\"\"select distinct HADM_ID, \n",
    "case when ICD_GROUP = 'diseases of the circulatory system' then 'D1'\n",
    "when ICD_GROUP = 'external causes of injury and supplemental classification' then 'D2'\n",
    "when ICD_GROUP = 'endocrine, nutritional and metabolic diseases, and immunity disorders' then 'D3'\n",
    "when ICD_GROUP = 'diseases of the respiratory system' then 'D4'\n",
    "when ICD_GROUP = 'injury and poisoning' then 'D5' end as ICD_GROUP_ID\n",
    "from DIAGNOSES_ICD_WITH_GROUPING \n",
    "where ICD_GROUP in ('diseases of the circulatory system',\n",
    "'external causes of injury and supplemental classification',\n",
    "'endocrine, nutritional and metabolic diseases, and immunity disorders',\n",
    "'diseases of the respiratory system',\n",
    "'injury and poisoning')\"\"\")\n",
    "DIAGNOSIS_GROUPING_PIVOT_DF = DIAGNOSIS_GROUPING_DF.groupBy('HADM_ID').pivot('ICD_GROUP_ID').count().fillna(0)\n",
    "DIAGNOSIS_GROUPING_PIVOT_DF.createOrReplaceTempView('DIAGNOSIS_GROUPING_PIVOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+\n",
      "|HADM_ID| D1| D2| D3| D4| D5|\n",
      "+-------+---+---+---+---+---+\n",
      "| 153296|  1|  0|  1|  0|  0|\n",
      "| 109960|  1|  1|  0|  0|  0|\n",
      "| 155280|  1|  1|  1|  1|  0|\n",
      "| 144037|  1|  1|  1|  0|  0|\n",
      "| 149907|  0|  1|  0|  1|  1|\n",
      "+-------+---+---+---+---+---+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from DIAGNOSIS_GROUPING_PIVOT limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+\n",
      "|HADM_ID| D1| D2| D3| D4| D5|\n",
      "+-------+---+---+---+---+---+\n",
      "| 166435|  1|  1|  1|  0|  1|\n",
      "| 125592|  1|  1|  1|  0|  1|\n",
      "| 100263|  1|  1|  1|  1|  1|\n",
      "| 179104|  1|  1|  1|  0|  0|\n",
      "| 118388|  1|  0|  1|  1|  0|\n",
      "+-------+---+---+---+---+---+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "DIAGNOSIS_GROUPING_PIVOT_DF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Combine all notes at HADM level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_DF = spark.sql(\"\"\"SELECT A.HADM_ID, B.D1, B.D2, B.D3, B.D4, B.D5, lower(A.TEXT) as TEXT_LOWER \n",
    "from NOTEEVENTS A inner join DIAGNOSIS_GROUPING_PIVOT B on A.HADM_ID = B.HADM_ID\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Group all Text at HADM level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_GROUPED_DF = NOTEEVENTS_DF.groupby('HADM_ID','D1','D2','D3','D4','D5') \\\n",
    "                        .agg(F.concat_ws(\"\", F.collect_list(NOTEEVENTS_DF.TEXT_LOWER)).alias('TEXT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+--------------------+\n",
      "|HADM_ID| D1| D2| D3| D4| D5|                TEXT|\n",
      "+-------+---+---+---+---+---+--------------------+\n",
      "| 100010|  0|  0|  1|  0|  0|admission date:  ...|\n",
      "| 100140|  0|  1|  0|  1|  0|[**2117-6-17**] 1...|\n",
      "| 100227|  1|  1|  0|  1|  1|admission date:  ...|\n",
      "| 100263|  1|  1|  1|  1|  1|admission date:  ...|\n",
      "| 100320|  1|  1|  1|  0|  0|admission date:  ...|\n",
      "+-------+---+---+---+---+---+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "NOTEEVENTS_GROUPED_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HADM_ID: string (nullable = true)\n",
      " |-- D1: long (nullable = true)\n",
      " |-- D2: long (nullable = true)\n",
      " |-- D3: long (nullable = true)\n",
      " |-- D4: long (nullable = true)\n",
      " |-- D5: long (nullable = true)\n",
      " |-- TEXT: string (nullable = false)"
     ]
    }
   ],
   "source": [
    "NOTEEVENTS_GROUPED_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  ...                                                  4\n",
      "summary  count  ...                                                max\n",
      "HADM_ID  57632  ...                                             199999\n",
      "D1       57632  ...                                                  1\n",
      "D2       57632  ...                                                  1\n",
      "D3       57632  ...                                                  1\n",
      "D4       57632  ...                                                  1\n",
      "D5       57632  ...                                                  1\n",
      "TEXT     57632  ...  y\\nname:  [**known lastname 95474**], [**known...\n",
      "\n",
      "[8 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "NOTEEVENTS_GROUPED_DF.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Convert to Parquete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_GROUPED_DF.write.mode('overwrite').parquet(S3_URL+'/NOTEEVENTS_GROUPED_DIAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
