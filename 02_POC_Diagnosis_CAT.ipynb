{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept - Predict Top-5 diagnosis categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul> <li> D1: Diseases of the circulatory system </ul> </li>\n",
    "<ul> <li> D2: External causes of injury and supplemental classification </ul> </li>\n",
    "<ul> <li> D3: Endocrine, nutritional and metabolic diseases, and immunity disorders </ul> </li>\n",
    "<ul> <li> D4: Diseases of the respiratory system </ul> </li>\n",
    "<ul> <li> D5: Injury and poisoning </ul> </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre process data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kfpj179\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import collect_list\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"ClickThrough\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"ClickThrough\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_LIST = os.listdir(\"data\")\n",
    "for filename_LIST_ITEM in filename_LIST:\n",
    "    filename,fileformat = filename_LIST_ITEM.split('.')\n",
    "    exec(filename+'_DF = sqlContext.read.format(\"'+fileformat+'\").option(\"header\", \"true\").option(\"multiline\",True).'+\n",
    "         'option(\"escape\",'+\"'\"+'\"'+\"')\"+'.load(\"data/'+filename+'.csv\")')\n",
    "    exec(filename+'_DF.createOrReplaceTempView(\"'+filename+'\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Build HADM level diagnosis flag tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'java.lang.RuntimeException: java.io.IOException: (null) entry in command string: null chmod 0733 C:\\\\tmp\\\\hive;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.sql.\n: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.io.IOException: (null) entry in command string: null chmod 0733 C:\\tmp\\hive;\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: (null) entry in command string: null chmod 0733 C:\\tmp\\hive\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:185)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:118)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\t... 84 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0733 C:\\tmp\\hive\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:491)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:532)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:509)\r\n\tat org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:305)\r\n\tat org.apache.hadoop.hive.ql.exec.Utilities.createDirsWithPermission(Utilities.java:3679)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:597)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:554)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:508)\r\n\t... 99 more\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-67a9004d4120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mleft\u001b[0m \u001b[0mjoin\u001b[0m \u001b[0mD_ICD_DIAGNOSES\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mon\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mICD9_CODE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mICD9_CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \"\"\").createOrReplaceTempView('DIAGNOSES_ICD_WITH_GROUPING')\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \"\"\"\n\u001b[1;32m--> 767\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'java.lang.RuntimeException: java.io.IOException: (null) entry in command string: null chmod 0733 C:\\\\tmp\\\\hive;'"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select A.*, B.SHORT_TITLE, B.LONG_TITLE, \n",
    "case when substr(A.ICD9_CODE,1,1) in ('E','V') then 'external causes of injury and supplemental classification'\n",
    "when substr(A.ICD9_CODE,1,3) between 001 and 139 then 'infectious and parasitic diseases'\n",
    "when substr(A.ICD9_CODE,1,3) between 140 and 239 then 'neoplasms'\n",
    "when substr(A.ICD9_CODE,1,3) between 240 and 279 then 'endocrine, nutritional and metabolic diseases, and immunity disorders'\n",
    "when substr(A.ICD9_CODE,1,3) between 280 and 289 then 'diseases of the blood and blood-forming organs'\n",
    "when substr(A.ICD9_CODE,1,3) between 290 and 319 then 'mental disorders'\n",
    "when substr(A.ICD9_CODE,1,3) between 320 and 389 then 'diseases of the nervous system and sense organs'\n",
    "when substr(A.ICD9_CODE,1,3) between 390 and 459 then 'diseases of the circulatory system'\n",
    "when substr(A.ICD9_CODE,1,3) between 460 and 519 then 'diseases of the respiratory system'\n",
    "when substr(A.ICD9_CODE,1,3) between 520 and 579 then 'diseases of the digestive system'\n",
    "when substr(A.ICD9_CODE,1,3) between 580 and 629 then 'diseases of the genitourinary system'\n",
    "when substr(A.ICD9_CODE,1,3) between 630 and 679 then 'complications of pregnancy, childbirth, and the puerperium'\n",
    "when substr(A.ICD9_CODE,1,3) between 680 and 709 then 'diseases of the skin and subcutaneous tissue'\n",
    "when substr(A.ICD9_CODE,1,3) between 710 and 739 then 'diseases of the musculoskeletal system and connective tissue'\n",
    "when substr(A.ICD9_CODE,1,3) between 740 and 759 then 'congenital anomalies'\n",
    "when substr(A.ICD9_CODE,1,3) between 760 and 779 then 'certain conditions originating in the perinatal period'\n",
    "when substr(A.ICD9_CODE,1,3) between 780 and 799 then 'symptoms, signs, and ill-defined conditions'\n",
    "when substr(A.ICD9_CODE,1,3) between 800 and 999 then 'injury and poisoning' \n",
    "end as ICD_GROUP\n",
    "from DIAGNOSES_ICD A\n",
    "left join D_ICD_DIAGNOSES B\n",
    "on A.ICD9_CODE = B.ICD9_CODE\n",
    "\"\"\").createOrReplaceTempView('DIAGNOSES_ICD_WITH_GROUPING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGNOSIS_GROUPING_DF = spark.sql(\"\"\"select distinct HADM_ID, \n",
    "case when ICD_GROUP = 'diseases of the circulatory system' then 'D1'\n",
    "when ICD_GROUP = 'external causes of injury and supplemental classification' then 'D2'\n",
    "when ICD_GROUP = 'endocrine, nutritional and metabolic diseases, and immunity disorders' then 'D3'\n",
    "when ICD_GROUP = 'diseases of the respiratory system' then 'D4'\n",
    "when ICD_GROUP = 'injury and poisoning' then 'D5' end as ICD_GROUP_ID\n",
    "from DIAGNOSES_ICD_WITH_GROUPING \n",
    "where ICD_GROUP in ('diseases of the circulatory system',\n",
    "'external causes of injury and supplemental classification',\n",
    "'endocrine, nutritional and metabolic diseases, and immunity disorders',\n",
    "'diseases of the respiratory system',\n",
    "'injury and poisoning')\"\"\")\n",
    "DIAGNOSIS_GROUPING_PIVOT_DF = DIAGNOSIS_GROUPING_DF.groupBy('HADM_ID').pivot('ICD_GROUP_ID').count().fillna(0)\n",
    "DIAGNOSIS_GROUPING_PIVOT_DF.createOrReplaceTempView('DIAGNOSIS_GROUPING_PIVOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select * from DIAGNOSIS_GROUPING_PIVOT limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGNOSIS_GROUPING_PIVOT_DF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Combine all notes at HADM level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_DF = spark.sql(\"\"\"SELECT A.HADM_ID, B.D1, B.D2, B.D3, B.D4, B.D5, lower(A.TEXT) as TEXT_LOWER \n",
    "from NOTEEVENTS A inner join DIAGNOSIS_GROUPING_PIVOT B on A.HADM_ID = B.HADM_ID\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Group all Text at HADM level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_GROUPED_DF = NOTEEVENTS_DF.groupby('HADM_ID','D1','D2','D3','D4','D5') \\\n",
    "                        .agg(F.concat_ws(\"\", F.collect_list(NOTEEVENTS_DF.TEXT_LOWER)).alias('TEXT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_GROUPED_DF.cache()\n",
    "NOTEEVENTS_GROUPED_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_GROUPED_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_GROUPED_DF.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Convert to lowercase and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTES_RDD = NOTEEVENTS_GROUPED_DF.select('HADM_ID','TEXT').rdd\n",
    "def TokenizeFunct(x):\n",
    "    return (x[0], nltk.word_tokenize(re.sub('\\n',' ',re.sub(r'\\[\\*\\*.+\\*\\*\\]','xxx',x[1]))))\n",
    "NOTES_RDD = NOTES_RDD.map(TokenizeFunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Remove stopwords, numbers and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWordsFunct(x):\n",
    "    stop_words=set((stopwords.words('english')))\n",
    "    filteredSentence = [w for w in x[1] if (w not in stop_words)]\n",
    "    return (x[0], filteredSentence)\n",
    "NOTES_RDD = NOTES_RDD.map(removeStopWordsFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuationsFunct(x):\n",
    "    list_punct=list(string.punctuation)\n",
    "    filtered = [''.join(c for c in s if c not in list_punct) for s in x[1]] \n",
    "    filtered_space = [s for s in filtered if s] #remove empty space \n",
    "    #filtered_num = [s for s in filtered_space if not(s.isnumeric())]\n",
    "    return (x[0], filtered_space)\n",
    "NOTES_RDD = NOTES_RDD.map(removePunctuationsFunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizationFunct(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    finalLem = [lemmatizer.lemmatize(s) for s in x[1] if s not in ['']]\n",
    "    return (x[0], finalLem)\n",
    "NOTES_RDD = NOTES_RDD.map(lemmatizationFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100010',\n",
       "  ['admission',\n",
       "   'date',\n",
       "   'xxx',\n",
       "   'date',\n",
       "   'birth',\n",
       "   'xxx',\n",
       "   'sex',\n",
       "   'f',\n",
       "   'service',\n",
       "   'urology',\n",
       "   'allergy',\n",
       "   'penicillin',\n",
       "   'attending',\n",
       "   'xxx',\n",
       "   'chief',\n",
       "   'complaint',\n",
       "   'gross',\n",
       "   'hematuria',\n",
       "   '50pound',\n",
       "   'weight',\n",
       "   'loss',\n",
       "   'major',\n",
       "   'surgical',\n",
       "   'invasive',\n",
       "   'procedure',\n",
       "   'open',\n",
       "   'left',\n",
       "   'radical',\n",
       "   'nephrectomy',\n",
       "   'history',\n",
       "   'present',\n",
       "   'illness',\n",
       "   '54',\n",
       "   'yo',\n",
       "   'female',\n",
       "   'wlarge',\n",
       "   'left',\n",
       "   'renal',\n",
       "   'mass',\n",
       "   'sp',\n",
       "   'mediastinoscopy',\n",
       "   'showing',\n",
       "   'metastatic',\n",
       "   'rcc',\n",
       "   'xxx',\n",
       "   'previously',\n",
       "   'healthy',\n",
       "   'patientn',\n",
       "   'two',\n",
       "   'year',\n",
       "   'ago',\n",
       "   'noted',\n",
       "   'one',\n",
       "   'episode',\n",
       "   'gross',\n",
       "   'hematuria',\n",
       "   'quickly',\n",
       "   'resolved',\n",
       "   'recently',\n",
       "   '50pound',\n",
       "   'weight',\n",
       "   'loss',\n",
       "   '210',\n",
       "   '160',\n",
       "   'pound',\n",
       "   'past',\n",
       "   'six',\n",
       "   'month',\n",
       "   'noted',\n",
       "   'fullness',\n",
       "   'left',\n",
       "   'upper',\n",
       "   'quadrant',\n",
       "   'actual',\n",
       "   'pain',\n",
       "   'requires',\n",
       "   'occasional',\n",
       "   'tylenol',\n",
       "   'imaging',\n",
       "   'performed',\n",
       "   'revealed',\n",
       "   'large',\n",
       "   'left',\n",
       "   'renal',\n",
       "   'mass',\n",
       "   'consistent',\n",
       "   'renal',\n",
       "   'cell',\n",
       "   'carcinoma',\n",
       "   'xxx',\n",
       "   'underwent',\n",
       "   'mediastinoscopy',\n",
       "   'lymph',\n",
       "   'node',\n",
       "   'biopsy',\n",
       "   'consistent',\n",
       "   'metastatic',\n",
       "   'carcinoma',\n",
       "   'noted',\n",
       "   'severe',\n",
       "   'fatigue',\n",
       "   'although',\n",
       "   'walk',\n",
       "   'least',\n",
       "   '10',\n",
       "   'minute',\n",
       "   'performance',\n",
       "   'status',\n",
       "   'lower',\n",
       "   'normal',\n",
       "   'probably',\n",
       "   'ecog',\n",
       "   '1',\n",
       "   'gross',\n",
       "   'hematuria',\n",
       "   'urinary',\n",
       "   'infection',\n",
       "   'urinary',\n",
       "   'symptom',\n",
       "   'occasional',\n",
       "   'night',\n",
       "   'sweat',\n",
       "   'fever',\n",
       "   'occasional',\n",
       "   'cough',\n",
       "   'hemoptysis',\n",
       "   'past',\n",
       "   'medical',\n",
       "   'history',\n",
       "   'pmhpsh',\n",
       "   'mediastinoscopy',\n",
       "   'xxx',\n",
       "   'ectopic',\n",
       "   'pregnancy',\n",
       "   'right',\n",
       "   'knee',\n",
       "   'surgery',\n",
       "   'xxx',\n",
       "   'laparoscopy',\n",
       "   'evaluate',\n",
       "   'multiple',\n",
       "   'miscarriage',\n",
       "   'gerd',\n",
       "   'social',\n",
       "   'history',\n",
       "   '40packyear',\n",
       "   'smoking',\n",
       "   'history',\n",
       "   'quit',\n",
       "   'xxx',\n",
       "   'alcohol',\n",
       "   'housewife',\n",
       "   '12yearold',\n",
       "   'daughter',\n",
       "   'family',\n",
       "   'history',\n",
       "   'family',\n",
       "   'history',\n",
       "   'unremarkable',\n",
       "   'physical',\n",
       "   'exam',\n",
       "   'nad',\n",
       "   'ox3',\n",
       "   'respiratory',\n",
       "   'distress',\n",
       "   'abd',\n",
       "   'soft',\n",
       "   'nt',\n",
       "   'nd',\n",
       "   'left',\n",
       "   'flank',\n",
       "   'incision',\n",
       "   'cdi',\n",
       "   'ct',\n",
       "   'site',\n",
       "   'cdi',\n",
       "   'ext',\n",
       "   'cce',\n",
       "   'pertinent',\n",
       "   'result',\n",
       "   'xxx',\n",
       "   '0656am',\n",
       "   'blood',\n",
       "   'wbc74',\n",
       "   'rbc354',\n",
       "   'hgb92',\n",
       "   'hct281',\n",
       "   'mcv79',\n",
       "   'mch261',\n",
       "   'mchc329',\n",
       "   'rdw165',\n",
       "   'plt',\n",
       "   'ct281',\n",
       "   'xxx',\n",
       "   '0656am',\n",
       "   'blood',\n",
       "   'glucose110',\n",
       "   'urean11',\n",
       "   'creat13',\n",
       "   'na136',\n",
       "   'k44',\n",
       "   'cl104',\n",
       "   'hco327',\n",
       "   'angap9',\n",
       "   'brief',\n",
       "   'hospital',\n",
       "   'course',\n",
       "   'patient',\n",
       "   'admitted',\n",
       "   'urology',\n",
       "   'undergoing',\n",
       "   'open',\n",
       "   'left',\n",
       "   'radical',\n",
       "   'nephrectomy',\n",
       "   'concerning',\n",
       "   'intraoperative',\n",
       "   'event',\n",
       "   'occurred',\n",
       "   'please',\n",
       "   'see',\n",
       "   'dictated',\n",
       "   'operative',\n",
       "   'note',\n",
       "   'detail',\n",
       "   'patient',\n",
       "   'received',\n",
       "   'perioperative',\n",
       "   'antibiotic',\n",
       "   'prophylaxis',\n",
       "   'patient',\n",
       "   'transferred',\n",
       "   'sicu',\n",
       "   'pacu',\n",
       "   'due',\n",
       "   'hypotension',\n",
       "   'evening',\n",
       "   'pod',\n",
       "   '0',\n",
       "   'hct',\n",
       "   '25',\n",
       "   'transfused',\n",
       "   '1',\n",
       "   'unit',\n",
       "   'overnight',\n",
       "   'pressor',\n",
       "   'keep',\n",
       "   'sbp',\n",
       "   '90',\n",
       "   'epidural',\n",
       "   'lowered',\n",
       "   'changed',\n",
       "   'plain',\n",
       "   'bupivicaine',\n",
       "   'dilaudid',\n",
       "   'pca',\n",
       "   'bp',\n",
       "   'improved',\n",
       "   'pod',\n",
       "   '1',\n",
       "   'pod',\n",
       "   '1',\n",
       "   'pain',\n",
       "   'well',\n",
       "   'controlled',\n",
       "   'pca',\n",
       "   'epidural',\n",
       "   'hydrated',\n",
       "   'urine',\n",
       "   'output',\n",
       "   '30cchour',\n",
       "   'provided',\n",
       "   'pneumoboots',\n",
       "   'incentive',\n",
       "   'spirometry',\n",
       "   'prophylaxis',\n",
       "   'ambulated',\n",
       "   'pod',\n",
       "   '1',\n",
       "   'pressor',\n",
       "   'also',\n",
       "   'weaned',\n",
       "   'chest',\n",
       "   'tube',\n",
       "   'removed',\n",
       "   'complication',\n",
       "   'cxr',\n",
       "   'showed',\n",
       "   'pneumothorax',\n",
       "   'pod2',\n",
       "   'transferred',\n",
       "   'floor',\n",
       "   'diet',\n",
       "   'advanced',\n",
       "   'pod3',\n",
       "   'epidural',\n",
       "   'foley',\n",
       "   'removed',\n",
       "   'transitioned',\n",
       "   'oral',\n",
       "   'pain',\n",
       "   'med',\n",
       "   'remainder',\n",
       "   'hospital',\n",
       "   'course',\n",
       "   'relatively',\n",
       "   'unremarkable',\n",
       "   'patient',\n",
       "   'discharged',\n",
       "   'stable',\n",
       "   'condition',\n",
       "   'pod',\n",
       "   '4',\n",
       "   'eating',\n",
       "   'well',\n",
       "   'ambulating',\n",
       "   'independently',\n",
       "   'voiding',\n",
       "   'without',\n",
       "   'difficulty',\n",
       "   'pain',\n",
       "   'control',\n",
       "   'oral',\n",
       "   'analgesic',\n",
       "   'exam',\n",
       "   'incision',\n",
       "   'clean',\n",
       "   'dry',\n",
       "   'intact',\n",
       "   'evidence',\n",
       "   'hematoma',\n",
       "   'collection',\n",
       "   'infection',\n",
       "   'patient',\n",
       "   'given',\n",
       "   'explicit',\n",
       "   'instruction',\n",
       "   'followup',\n",
       "   'clinic',\n",
       "   'drxxx',\n",
       "   'medication',\n",
       "   'admission',\n",
       "   'tylenol',\n",
       "   'prn',\n",
       "   'pepcid',\n",
       "   'prn',\n",
       "   'discharge',\n",
       "   'medication',\n",
       "   '1',\n",
       "   'docusate',\n",
       "   'sodium',\n",
       "   '100',\n",
       "   'mg',\n",
       "   'capsule',\n",
       "   'sig',\n",
       "   'one',\n",
       "   '1',\n",
       "   'capsule',\n",
       "   'po',\n",
       "   'bid',\n",
       "   '2',\n",
       "   'time',\n",
       "   'day',\n",
       "   'disp',\n",
       "   '60',\n",
       "   'capsule',\n",
       "   'refill',\n",
       "   '2',\n",
       "   '2',\n",
       "   'oxycodone',\n",
       "   '5',\n",
       "   'mg',\n",
       "   'tablet',\n",
       "   'sig',\n",
       "   '12',\n",
       "   'tablet',\n",
       "   'po',\n",
       "   'q4h',\n",
       "   'every',\n",
       "   '4',\n",
       "   'hour',\n",
       "   'needed',\n",
       "   'pain',\n",
       "   'disp',\n",
       "   '40',\n",
       "   'tablet',\n",
       "   'refill',\n",
       "   '0',\n",
       "   '3',\n",
       "   'famotidine',\n",
       "   '20',\n",
       "   'mg',\n",
       "   'tablet',\n",
       "   'sig',\n",
       "   'one',\n",
       "   '1',\n",
       "   'tablet',\n",
       "   'po',\n",
       "   'q24h',\n",
       "   'every',\n",
       "   '24',\n",
       "   'hour',\n",
       "   'needed',\n",
       "   'heartburn',\n",
       "   '4',\n",
       "   'acetaminophen',\n",
       "   '500',\n",
       "   'mg',\n",
       "   'tablet',\n",
       "   'sig',\n",
       "   '12',\n",
       "   'tablet',\n",
       "   'po',\n",
       "   'q8h',\n",
       "   'every',\n",
       "   '8',\n",
       "   'hour',\n",
       "   'needed',\n",
       "   'fever',\n",
       "   'pain',\n",
       "   '5',\n",
       "   'ibuprofen',\n",
       "   '600',\n",
       "   'mg',\n",
       "   'tablet',\n",
       "   'sig',\n",
       "   'one',\n",
       "   '1',\n",
       "   'tablet',\n",
       "   'po',\n",
       "   'every',\n",
       "   'six',\n",
       "   '6',\n",
       "   'hour',\n",
       "   'needed',\n",
       "   'pain',\n",
       "   'discharge',\n",
       "   'disposition',\n",
       "   'home',\n",
       "   'discharge',\n",
       "   'diagnosis',\n",
       "   'metastatic',\n",
       "   'rccleft',\n",
       "   'renal',\n",
       "   'mass',\n",
       "   'sp',\n",
       "   'open',\n",
       "   'left',\n",
       "   'radical',\n",
       "   'nephrectomy',\n",
       "   'discharge',\n",
       "   'condition',\n",
       "   'stable',\n",
       "   'alert',\n",
       "   'oriented',\n",
       "   'ambulatory',\n",
       "   'discharge',\n",
       "   'instruction',\n",
       "   'you',\n",
       "   'may',\n",
       "   'shower',\n",
       "   'bathe',\n",
       "   'swim',\n",
       "   'immerse',\n",
       "   'incision',\n",
       "   'do',\n",
       "   'lift',\n",
       "   'anything',\n",
       "   'heavier',\n",
       "   'phone',\n",
       "   'book',\n",
       "   '10',\n",
       "   'pound',\n",
       "   'drive',\n",
       "   'seen',\n",
       "   'urologist',\n",
       "   'followup',\n",
       "   'tylenol',\n",
       "   'first',\n",
       "   'line',\n",
       "   'pain',\n",
       "   'medication',\n",
       "   'narcotic',\n",
       "   'pain',\n",
       "   'medication',\n",
       "   'prescribed',\n",
       "   'breakthough',\n",
       "   'pain',\n",
       "   '4',\n",
       "   'replace',\n",
       "   'tylenol',\n",
       "   'narcotic',\n",
       "   'pain',\n",
       "   'medication',\n",
       "   'max',\n",
       "   'daily',\n",
       "   'tylenol',\n",
       "   'dose',\n",
       "   '4gm',\n",
       "   'note',\n",
       "   'narcotic',\n",
       "   'pain',\n",
       "   'medication',\n",
       "   'also',\n",
       "   'contains',\n",
       "   'tylenol',\n",
       "   'acetaminophen',\n",
       "   'do',\n",
       "   'drive',\n",
       "   'drink',\n",
       "   'alcohol',\n",
       "   'taking',\n",
       "   'narcotic',\n",
       "   'colace',\n",
       "   'prescribed',\n",
       "   'avoid',\n",
       "   'post',\n",
       "   'surgical',\n",
       "   'constipation',\n",
       "   'constipation',\n",
       "   'related',\n",
       "   'narcotic',\n",
       "   'pain',\n",
       "   'medication',\n",
       "   'discontinue',\n",
       "   'loose',\n",
       "   'stool',\n",
       "   'diarrhea',\n",
       "   'develops',\n",
       "   'resume',\n",
       "   'home',\n",
       "   'medication',\n",
       "   'except',\n",
       "   'hold',\n",
       "   'aspirin',\n",
       "   'see',\n",
       "   'urologist',\n",
       "   'followup',\n",
       "   'if',\n",
       "   'fever',\n",
       "   '1015',\n",
       "   'f',\n",
       "   'vomiting',\n",
       "   'increased',\n",
       "   'redness',\n",
       "   'swelling',\n",
       "   'discharge',\n",
       "   'incision',\n",
       "   'call',\n",
       "   'doctor',\n",
       "   'go',\n",
       "   'nearest',\n",
       "   'er',\n",
       "   'do',\n",
       "   'eat',\n",
       "   'constipating',\n",
       "   'food',\n",
       "   '24',\n",
       "   'week',\n",
       "   'drink',\n",
       "   'plenty',\n",
       "   'fluid',\n",
       "   'followup',\n",
       "   'instruction',\n",
       "   'schedule',\n",
       "   'appointment',\n",
       "   'primary',\n",
       "   'care',\n",
       "   'physician',\n",
       "   'staple',\n",
       "   'removed',\n",
       "   'xxx',\n",
       "   'call',\n",
       "   'drxxx',\n",
       "   'followup',\n",
       "   'question',\n",
       "   'page',\n",
       "   'dr',\n",
       "   'xxx',\n",
       "   'completed',\n",
       "   'xxx',\n",
       "   '410',\n",
       "   'pm',\n",
       "   'chest',\n",
       "   'portable',\n",
       "   'ap',\n",
       "   '77',\n",
       "   'different',\n",
       "   'physician',\n",
       "   'xxx',\n",
       "   'reason',\n",
       "   'evaluate',\n",
       "   'ptx',\n",
       "   'chest',\n",
       "   'tube',\n",
       "   'removal',\n",
       "   'admitting',\n",
       "   'diagnosis',\n",
       "   'renal',\n",
       "   'mass',\n",
       "   'leftsda',\n",
       "   'xxx',\n",
       "   'medical',\n",
       "   'condition',\n",
       "   '54',\n",
       "   'year',\n",
       "   'old',\n",
       "   'woman',\n",
       "   'sp',\n",
       "   'radical',\n",
       "   'nephrectomy',\n",
       "   'reason',\n",
       "   'examination',\n",
       "   'evaluate',\n",
       "   'ptx',\n",
       "   'chest',\n",
       "   'tube',\n",
       "   'removal',\n",
       "   'final',\n",
       "   'report',\n",
       "   'indication',\n",
       "   '54yearold',\n",
       "   'female',\n",
       "   'status',\n",
       "   'post',\n",
       "   'radical',\n",
       "   'nephrectomy',\n",
       "   'evaluate',\n",
       "   'pneumothorax',\n",
       "   'chest',\n",
       "   'tube',\n",
       "   'removal',\n",
       "   'comparison',\n",
       "   'xxx',\n",
       "   'frontal',\n",
       "   'chest',\n",
       "   'radiograph',\n",
       "   'leftsided',\n",
       "   'chest',\n",
       "   'tube',\n",
       "   'removed',\n",
       "   'pneumothorax',\n",
       "   'moderate',\n",
       "   'left',\n",
       "   'lower',\n",
       "   'lobe',\n",
       "   'atelectasis',\n",
       "   'unchanged',\n",
       "   'right',\n",
       "   'lung',\n",
       "   'clear',\n",
       "   'right',\n",
       "   'internal',\n",
       "   'jugular',\n",
       "   'central',\n",
       "   'venous',\n",
       "   'line',\n",
       "   'unchanged'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTES_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTES_RDD.toDF(['HADM_ID', 'TEXT_TOKEN']).createOrReplaceTempView('NOTES_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 Combine diagnosis group flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[HADM_ID: string, D1: bigint, D2: bigint, D3: bigint, D4: bigint, D5: bigint, TEXT_TOKEN: array<string>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTES_TOKEN_DF = spark.sql(\"\"\"SELECT A.HADM_ID, B.D1, B.D2, B.D3, B.D4, B.D5, A.TEXT_TOKEN\n",
    "from NOTES_TOKEN A inner join DIAGNOSIS_GROUPING_PIVOT B on A.HADM_ID = B.HADM_ID\"\"\")\n",
    "NOTES_TOKEN_DF.cache()\n",
    "#NOTE_TOKEN_DF.write.parquet('C:/Users/kfpj179/Desktop/Final Project/data/mimic-train-1')\n",
    "#NOTE_TOKEN_DF.summary().toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+--------------------+\n",
      "|HADM_ID| D1| D2| D3| D4| D5|          TEXT_TOKEN|\n",
      "+-------+---+---+---+---+---+--------------------+\n",
      "| 100010|  0|  0|  1|  0|  0|[admission, date,...|\n",
      "| 100140|  0|  1|  0|  1|  0|[xxx, 1228, pm, c...|\n",
      "+-------+---+---+---+---+---+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NOTES_TOKEN_DF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HADM_ID: string (nullable = true)\n",
      " |-- D1: long (nullable = true)\n",
      " |-- D2: long (nullable = true)\n",
      " |-- D3: long (nullable = true)\n",
      " |-- D4: long (nullable = true)\n",
      " |-- D5: long (nullable = true)\n",
      " |-- TEXT_TOKEN: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NOTES_TOKEN_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HADM_ID</th>\n",
       "      <td>57632</td>\n",
       "      <td>149959.28055594114</td>\n",
       "      <td>28882.103401634144</td>\n",
       "      <td>100001</td>\n",
       "      <td>199999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>57632</td>\n",
       "      <td>0.735181843420322</td>\n",
       "      <td>0.44124015992805676</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>57632</td>\n",
       "      <td>0.7184550249861188</td>\n",
       "      <td>0.44975650293216807</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3</th>\n",
       "      <td>57632</td>\n",
       "      <td>0.6138950583009439</td>\n",
       "      <td>0.48685935191704993</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D4</th>\n",
       "      <td>57632</td>\n",
       "      <td>0.435417823431427</td>\n",
       "      <td>0.49581590135958314</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D5</th>\n",
       "      <td>57632</td>\n",
       "      <td>0.3905469183786785</td>\n",
       "      <td>0.4878771904822777</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0                   1                    2       3       4\n",
       "summary  count                mean               stddev     min     max\n",
       "HADM_ID  57632  149959.28055594114   28882.103401634144  100001  199999\n",
       "D1       57632   0.735181843420322  0.44124015992805676       0       1\n",
       "D2       57632  0.7184550249861188  0.44975650293216807       0       1\n",
       "D3       57632  0.6138950583009439  0.48685935191704993       0       1\n",
       "D4       57632   0.435417823431427  0.49581590135958314       0       1\n",
       "D5       57632  0.3905469183786785   0.4878771904822777       0       1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTES_TOKEN_DF.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "NOTES_TOKEN_DF = NOTES_TOKEN_DF.withColumn('D1',NOTES_TOKEN_DF['D1'].cast(IntegerType()))\n",
    "NOTES_TOKEN_DF = NOTES_TOKEN_DF.withColumn('D2',NOTES_TOKEN_DF['D2'].cast(IntegerType()))\n",
    "NOTES_TOKEN_DF = NOTES_TOKEN_DF.withColumn('D3',NOTES_TOKEN_DF['D3'].cast(IntegerType()))\n",
    "NOTES_TOKEN_DF = NOTES_TOKEN_DF.withColumn('D4',NOTES_TOKEN_DF['D4'].cast(IntegerType()))\n",
    "NOTES_TOKEN_DF = NOTES_TOKEN_DF.withColumn('D5',NOTES_TOKEN_DF['D5'].cast(IntegerType())).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+--------------------+\n",
      "|HADM_ID| D1| D2| D3| D4| D5|                TEXT|\n",
      "+-------+---+---+---+---+---+--------------------+\n",
      "| 100010|  0|  0|  1|  0|  0|admission date:  ...|\n",
      "| 100140|  0|  1|  0|  1|  0|[**2117-6-17**] 1...|\n",
      "| 100227|  1|  1|  0|  1|  1|admission date:  ...|\n",
      "| 100263|  1|  1|  1|  1|  1|admission date:  ...|\n",
      "| 100320|  1|  1|  1|  0|  0|admission date:  ...|\n",
      "+-------+---+---+---+---+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NOTEEVENTS_GROUPED_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3939.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\r\n\tat sun.reflect.GeneratedMethodAccessor401.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 411.0 failed 1 times, most recent failure: Lost task 5.0 in stage 411.0 (TID 3536, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\train\\_temporary\\0\\_temporary\\attempt_20200604081317_0411_m_000005_3536\\part-00005-198bcf14-46b8-4303-956e-1780fb27eb2f-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\r\n\t... 32 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\train\\_temporary\\0\\_temporary\\attempt_20200604081317_0411_m_000005_3536\\part-00005-198bcf14-46b8-4303-956e-1780fb27eb2f-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-227-6837ef1dff02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNOTES_TOKEN_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/tmp/train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#NOTEEVENTS_GROUPED_DF.coalesce(1).write.format(\"com.databricks.spark.csv\").mode('overwrite') \\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#        .option(\"header\", \"true\").option(\"nullValue\", \"0\").save(\"train.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#NOTEEVENTS_GROUPED_DF.write.csv('mycsv.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3939.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\r\n\tat sun.reflect.GeneratedMethodAccessor401.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 411.0 failed 1 times, most recent failure: Lost task 5.0 in stage 411.0 (TID 3536, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\train\\_temporary\\0\\_temporary\\attempt_20200604081317_0411_m_000005_3536\\part-00005-198bcf14-46b8-4303-956e-1780fb27eb2f-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\r\n\t... 32 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\train\\_temporary\\0\\_temporary\\attempt_20200604081317_0411_m_000005_3536\\part-00005-198bcf14-46b8-4303-956e-1780fb27eb2f-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "NOTES_TOKEN_DF.write.mode('overwrite').parquet('C:/tmp/train')\n",
    "#NOTEEVENTS_GROUPED_DF.coalesce(1).write.format(\"com.databricks.spark.csv\").mode('overwrite') \\\n",
    "#        .option(\"header\", \"true\").option(\"nullValue\", \"0\").save(\"train.csv\")\n",
    "#NOTEEVENTS_GROUPED_DF.write.csv('mycsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(HADM_ID='100010', D1=0, D2=0, D3=1, D4=0, D5=0, TEXT='admission date:  [**2109-12-10**]              discharge date:   [**2109-12-14**]\\n\\ndate of birth:  [**2055-6-3**]             sex:   f\\n\\nservice: urology\\n\\nallergies:\\npenicillins\\n\\nattending:[**first name3 (lf) 11304**]\\nchief complaint:\\ngross hematuria, 50-pound weight loss\\n\\nmajor surgical or invasive procedure:\\nopen left radical nephrectomy\\n\\n\\nhistory of present illness:\\n54 y/o female w/large left renal mass, s/p mediastinoscopy\\nshowing metastatic rcc.  [**known firstname **] is a previously healthy\\npatientn who, two years ago, noted one episode of gross\\nhematuria, which quickly resolved.  more recently, she has had a\\n50-pound weight loss from 210 to 160 pounds over the past six\\nmonths.  she has noted some\\nfullness in the left upper quadrant, but no actual pain and only\\nrequires very occasional tylenol for this.  imaging was\\nperformed, which revealed a very large left renal mass\\nconsistent with renal cell carcinoma.  on [**2109-11-6**], she\\nunderwent a mediastinoscopy with lymph node biopsy, which was\\nconsistent with metastatic carcinoma.  she has noted severe\\nfatigue and although she can walk at least 10 minutes, her\\nperformance status is lower than normal for her, probably ecog\\n1.  no gross hematuria.  no urinary infections or other urinary\\nsymptoms. she has occasional night sweats, no fever, occasional\\ncough, no hemoptysis.\\n\\npast medical history:\\npmh/psh:  mediastinoscopy [**2108**], ectopic pregnancies, right knee\\nsurgery, [**2104**], laparoscopy to evaluate for multiple\\nmiscarriages, gerd.\\n\\n\\nsocial history:\\na 40-pack-year smoking history, quit in [**2088**].  no alcohol.  she\\nis a housewife.  they have a 12-year-old daughter.\\n\\nfamily history:\\nfamily history is unremarkable.\\n\\nphysical exam:\\nnad, a&ox3\\nno respiratory distress\\nabd: soft, nt, nd, left flank incision: c/d/i, ct site: c/di\\next: no c/c/e\\n\\n\\npertinent results:\\n[**2109-12-13**] 06:56am blood wbc-7.4 rbc-3.54* hgb-9.2* hct-28.1*\\nmcv-79* mch-26.1* mchc-32.9 rdw-16.5* plt ct-281\\n[**2109-12-13**] 06:56am blood glucose-110* urean-11 creat-1.3* na-136\\nk-4.4 cl-104 hco3-27 angap-9\\n\\nbrief hospital course:\\npatient was admitted to urology after undergoing an open left\\nradical nephrectomy. no concerning intraoperative events\\noccurred; please see dictated operative note for details. the\\npatient received perioperative antibiotic prophylaxis. the\\npatient was transferred to the sicu from the pacu due to\\nhypotension.  on the evening of pod 0, hct was 25.  she was\\ntransfused 1 unit overnight and was on pressors to keep sbp>90.\\nher epidural was lowered and changed to plain bupivicaine and\\ndilaudid pca, and her bp improved by pod 1.  on pod 1, pain was\\nwell controlled on pca and epidural, hydrated for urine output\\n>30cc/hour, provided with pneumoboots and incentive spirometry\\nfor prophylaxis, and she ambulated once.  on pod 1, pressors\\nwere also weaned off, and chest tube was removed with no\\ncomplications.  cxr showed no pneumothorax. on pod2, she was\\ntransferred to the floor and diet was advanced.  on pod3,\\nepidural and foley was removed and she was transitioned to oral\\npain meds.  the remainder of the hospital course was relatively\\nunremarkable. the patient was discharged in stable condition on\\npod 4, eating well, ambulating independently, voiding without\\ndifficulty, and with pain control on oral analgesics. on exam,\\nincision was clean, dry, and intact, with no evidence of\\nhematoma collection or infection. the patient was given explicit\\ninstructions to follow-up in clinic with dr.[**last name (stitle) 3748**].\\n\\nmedications on admission:\\ntylenol prn\\npepcid prn\\n\\ndischarge medications:\\n1. docusate sodium 100 mg capsule sig: one (1) capsule po bid (2\\ntimes a day).\\ndisp:*60 capsule(s)* refills:*2*\\n2. oxycodone 5 mg tablet sig: 1-2 tablets po q4h (every 4 hours)\\nas needed for pain.\\ndisp:*40 tablet(s)* refills:*0*\\n3. famotidine 20 mg tablet sig: one (1) tablet po q24h (every 24\\nhours) as needed for heartburn.\\n4. acetaminophen 500 mg tablet sig: 1-2 tablets po q8h (every 8\\nhours) as needed for fever, pain.\\n5. ibuprofen 600 mg tablet sig: one (1) tablet po every six (6)\\nhours as needed for pain.\\n\\n\\ndischarge disposition:\\nhome\\n\\ndischarge diagnosis:\\nmetastatic rcc/left renal mass s/p open left radical nephrectomy\\n\\n\\ndischarge condition:\\nstable\\nalert and oriented\\nambulatory\\n\\n\\ndischarge instructions:\\n-you may shower but do not bathe, swim or immerse your incision.\\n\\n-do not lift anything heavier than a phone book (10 pounds) or\\ndrive until you are seen by your urologist in follow-up\\n\\n-tylenol should be your first line pain medication, a narcotic\\npain medication has been prescribed for breakthough pain >4.\\nreplace tylenol with narcotic pain medication.  max daily\\ntylenol dose is 4gm, note that narcotic pain medication also\\ncontains tylenol (acetaminophen)\\n\\n-do not drive or drink alcohol while taking narcotics\\n\\n-colace has been prescribed to avoid post surgical constipation\\nand constipation related to narcotic pain medication,\\ndiscontinue if loose stool or diarrhea develops.\\n\\n-resume all of your home medications, except hold aspirin until\\nyou see your urologist in follow-up\\n\\n\\n-if you have fevers > 101.5 f, vomiting, or increased redness,\\nswelling, or discharge from your incision, call your doctor or\\ngo to the nearest er\\n\\n-do not eat constipating foods for 2-4 weeks, drink plenty of\\nfluids\\n\\nfollowup instructions:\\nschedule an appointment with your primary care physician to have\\nyour staples removed on [**2109-12-20**].\\n\\ncall dr.[**name (ni) 11306**] office ([**telephone/fax (1) 8791**];for follow-up and if you\\nhave any questions (page dr. [**last name (stitle) 3748**] at [**telephone/fax (1) 2756**]).\\n\\n\\n\\ncompleted by:[**2109-12-14**][**2109-12-11**] 4:10 pm\\n chest (portable ap); -77 by different physician                 [**name initial (pre) 26**] # [**clip number (radiology) 9848**]\\n reason: evaluate for ptx after chest tube removal\\n admitting diagnosis: renal mass left/sda\\n ______________________________________________________________________________\\n [**hospital 2**] medical condition:\\n  54 year old woman s/p radical nephrectomy\\n reason for this examination:\\n  evaluate for ptx after chest tube removal\\n ______________________________________________________________________________\\n                                 final report\\n indication:  54-year-old female status post radical nephrectomy, evaluate for\\n pneumothorax after chest tube removal.\\n\\n comparison:  [**2109-12-11**].\\n\\n frontal chest radiograph:  a left-sided chest tube has been removed and there\\n is no pneumothorax.  moderate left lower lobe atelectasis is unchanged.  the\\n right lung is clear.  right internal jugular central venous line is unchanged.\\n\\n\\n')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTEEVENTS_GROUPED_DF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
